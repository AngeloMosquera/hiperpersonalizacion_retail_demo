"use client"

import { useState, useRef, useEffect, useMemo, useCallback } from "react"
import { RealtimeAgent, RealtimeAgentRef } from "./realtime-agent"
import { useRouter, usePathname } from "next/navigation"
import { useTransferStore } from '@/lib/store/transfer-store'
import { useCreditCardStore } from '@/lib/store/credit-card-store'
import { getContextForPath } from '@/lib/conversation-contexts'
import { AudioRecorder } from './audio-recorder'

// Logger utility
const Logger = {
  info: (message: string, data?: any) => {
    console.log(`[VirtualAgent] ‚ÑπÔ∏è ${message}`, data || '')
  },
  error: (message: string, error?: any) => {
    console.error(`[VirtualAgent] ‚ùå ${message}`, error || '')
  },
  warn: (message: string, data?: any) => {
    console.warn(`[VirtualAgent] ‚ö†Ô∏è ${message}`, data || '')
  },
  debug: (message: string, data?: any) => {
    console.debug(`[VirtualAgent] üîç ${message}`, data || '')
  },
  success: (message: string, data?: any) => {
    console.log(`[VirtualAgent] ‚úÖ ${message}`, data || '')
  }
}

export function VirtualAgent() {
  const [isStreamReady, setIsStreamReady] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [connectionState, setConnectionState] = useState<'connecting' | 'connected' | 'error' | 'disconnected'>('disconnected')
  const [isMicrophoneActive, setIsMicrophoneActive] = useState(() => {
    // Recuperar estado del micr√≥fono del localStorage
    if (typeof window !== 'undefined') {
      const saved = localStorage.getItem('microphoneActive')
      return saved === 'true'
    }
    return false
  })
  const [isRecording, setIsRecording] = useState(false)
  const [navigationDetected, setNavigationDetected] = useState<string | null>(null)
  const streamingAgentRef = useRef<RealtimeAgentRef>(null)
  const router = useRouter()
  const pathname = usePathname()
  
  // Make currentContext reactive to pathname changes
  const currentContext = useMemo(() => {
    const context = getContextForPath(pathname)
    // DEBUG: Log current context detection
    console.log('=== VIRTUAL AGENT CONTEXT DEBUG ===')
    console.log('Current pathname:', pathname)
    console.log('Detected context ID:', context.id)
    console.log('Detected context name:', context.name)
    console.log('===================================')
    return context
  }, [pathname])
  
  // Log when currentContext changes to verify the fix
  useEffect(() => {
    Logger.info('Context updated', {
      pathname,
      contextId: currentContext.id,
      contextName: currentContext.name
    })
  }, [currentContext.id, pathname])
  
  const isMountedRef = useRef(true)
  const setTransferData = useTransferStore((state) => state.setTransferData)
  const setCreditCardData = useCreditCardStore((state) => state.setCreditCardData)
  
  // Ref para evitar procesar el mismo texto m√∫ltiples veces
  const lastProcessedTextRef = useRef<string>('')
  const isProcessingRef = useRef(false)

  const handleStreamReady = () => {
    if (!isMountedRef.current) return
    Logger.info('Conexi√≥n de voz establecida')
    setIsStreamReady(true)
    setConnectionState('connected')
    setError(null) // Clear any previous errors when connection is successful
    Logger.success('Sistema de voz directa listo')
  }

  const handleStreamError = (error: string) => {
    if (!isMountedRef.current) return
    
    // In development, WebRTC errors during strict mode are expected
    if (process.env.NODE_ENV === 'development' && 
        (error.includes('setRemoteDescription') || error.includes('m-lines') || error.includes('inicializar'))) {
      Logger.warn('Error de desarrollo detectado (React Strict Mode), ignorando', { error })
      return
    }
    
    Logger.error('Error en el stream', { error })
    setError(error)
    setConnectionState('error')
  }

  const handleMicrophoneToggle = () => {
    Logger.info('handleMicrophoneToggle llamado', { 
      currentState: isMicrophoneActive,
      hasRef: !!streamingAgentRef.current 
    })
    
    if (streamingAgentRef.current) {
      const newState = streamingAgentRef.current.toggleMicrophone()
      Logger.info('Estado del micr√≥fono cambiado', { 
        oldState: isMicrophoneActive, 
        newState 
      })
      setIsMicrophoneActive(newState)
      
      // Guardar estado en localStorage
      if (typeof window !== 'undefined') {
        localStorage.setItem('microphoneActive', newState.toString())
        Logger.info('Estado del micr√≥fono guardado en localStorage', { newState })
      }
    } else {
      Logger.warn('streamingAgentRef.current no disponible')
    }
  }

  const handleUserTranscription = (text: string) => {
    Logger.info('Transcripci√≥n del usuario recibida para detecci√≥n de intenci√≥n', { text })
    // Solo usar la transcripci√≥n para detectar intenci√≥n de navegaci√≥n
    // handleIntentDetection(text, 'user')
  }

  const handleIntentDetection = useCallback(async (text: string, source: 'user' | 'agent' = 'user') => {
    if (!isMountedRef.current) {
      Logger.warn('Componente no montado, ignorando detecci√≥n de intenci√≥n')
      return
    }
    
    Logger.info('Iniciando detecci√≥n de intenci√≥n', { 
      text, 
      source,
      isProcessing: isProcessingRef.current,
      lastProcessed: lastProcessedTextRef.current,
      isMounted: isMountedRef.current 
    })
    
    // Evitar procesar el mismo texto m√∫ltiples veces para intenci√≥n
    // if (lastProcessedTextRef.current === text || isProcessingRef.current) {
    //   Logger.info('Texto ya procesado para intenci√≥n, ignorando', { 
    //     text, 
    //     lastProcessed: lastProcessedTextRef.current,
    //     isProcessing: isProcessingRef.current 
    //   })
    //   return
    // }
    
    isProcessingRef.current = true
    lastProcessedTextRef.current = text
    
    Logger.info('Estado de procesamiento actualizado', { 
      isProcessing: isProcessingRef.current,
      lastProcessed: lastProcessedTextRef.current 
    })

    Logger.info('ContextId', { contextId: currentContext.id })
    
    // DEBUG: Verify current context is up to date
    console.log('=== USECALLBACK CONTEXT CHECK ===')
    console.log('Current context in handleIntentDetection:', currentContext.id)
    console.log('Current context name:', currentContext.name)
    console.log('==================================')
    
    try {
      Logger.debug('Detectando intenci√≥n de navegaci√≥n...', { text, source })
      
      // DEBUG: Log what we're sending to the API
      console.log('=== SENDING TO API ===')
      console.log('Text:', text)
      console.log('ContextId being sent:', currentContext.id)
      console.log('Context name:', currentContext.name)
      console.log('API URL: /api/openai/intent-detection')
      console.log('======================')

      if (currentContext.id === 'transfers' || currentContext.id === 'transfers_form') {
        const transferData = await fetch('/api/openai/transfers', { 
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            text: text,
          })
        })

        if (transferData.ok) {
          const transferResult = await transferData.json()
          if (transferResult.response && transferResult.response.response === true) {
            // Update transfer store with the received data
            const { nombreDestinatario, amount } = transferResult.response
            
            setTransferData({
              nombreDestinatario: nombreDestinatario || '',
              amount: amount || 0,
              description: '', // Keep existing description or set to empty
              response: 'success' // Indicate successful data extraction
            })
            
            Logger.info('Transfer data received and store updated', {
              nombreDestinatario,
              amount,
              transferResult
            })
            
            // Navigate to transfer form page with the extracted data
            setTimeout(() => {
              if (isMountedRef.current) {
                router.push('/transfers_form')
                Logger.success('Navigated to transfer form with extracted data')
              }
            }, 500) // Small delay to allow store update
          } else if (transferResult.response && transferResult.response.response === false) {
            Logger.info('Transfer data extraction was not successful - insufficient information provided')
          }
        }
      }
      
      const intentResponse = await fetch('/api/openai/intent-detection', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          text: text,
          contextId: currentContext.id
        })
      })

      if (intentResponse.ok) {
        const intentData = await intentResponse.json()
        Logger.debug('Respuesta de detecci√≥n de intenci√≥n', intentData)

        if (intentData.success && intentData.intent.hasNavigationIntent && intentData.intent.targetPage) {
          Logger.info('Intenci√≥n de navegaci√≥n detectada', {
            source,
            targetPage: intentData.intent.targetPage,
            confidence: intentData.intent.confidence,
            reasoning: intentData.intent.reasoning,
            text: text
          })
          
          // Mostrar indicador de navegaci√≥n
          setNavigationDetected(intentData.intent.targetPage)
          
          // Navegar autom√°ticamente despu√©s de una breve pausa
          setTimeout(() => {
            if (isMountedRef.current) {
              router.push(intentData.intent.targetPage)
              setNavigationDetected(null)
              Logger.success('Navegaci√≥n por intenci√≥n ejecutada')
            }
          }, 1500) // Pausa para mostrar el indicador
        } else {
          Logger.debug('No se detect√≥ intenci√≥n de navegaci√≥n', { 
            intent: intentData.intent,
            text: text 
          })
        }
      } else {
        Logger.warn('Error al detectar intenci√≥n', { 
          status: intentResponse.status,
          text: text 
        })
      }
    } catch (err) {
      if (isMountedRef.current) {
        Logger.error('Error al detectar intenci√≥n', err)
      }
    } finally {
      Logger.info('Finalizando detecci√≥n de intenci√≥n, programando limpieza')
      // Limpiar el estado de procesamiento despu√©s de un delay
      setTimeout(() => {
        if (isMountedRef.current) {
          isProcessingRef.current = false
          lastProcessedTextRef.current = ''
          Logger.info('Estado de procesamiento limpiado')
        } else {
          Logger.warn('Componente desmontado durante limpieza')
        }
      }, 2000) // 2 segundos de delay para evitar procesamiento inmediato
    }
  }, [currentContext, isMountedRef])

  const handleRecordingStateChange = (isRecording: boolean) => {
    setIsRecording(isRecording)
    Logger.info('Estado de grabaci√≥n cambiado', { isRecording })
  }

  // Event listener para respuestas del agente
  useEffect(() => {
    const handleAgentResponse = (event: CustomEvent<{ text: string }>) => {
      const agentText = event.detail.text
      Logger.info('Respuesta del agente recibida para detecci√≥n de intenci√≥n', { text: agentText })
      
      // Pasar la respuesta del agente tambi√©n a handleIntentDetection
      handleIntentDetection(agentText, 'agent')
    }
    
    window.addEventListener('agentTextResponse', handleAgentResponse)
    
    return () => {
      window.removeEventListener('agentTextResponse', handleAgentResponse)
    }
  }, [handleIntentDetection])

  // Log cuando el componente se monta
  useEffect(() => {
    Logger.info('Componente VirtualAgent montado')
    isMountedRef.current = true
    
    Logger.info('Sistema configurado para detecci√≥n de intenci√≥n de usuario y agente')
    
    return () => {
      Logger.info('Componente VirtualAgent desmontado')
      isMountedRef.current = false
      isProcessingRef.current = false
      lastProcessedTextRef.current = ''
      Logger.info('Estado limpiado')
    }
  }, [])

  // Log cuando cambia el estado de error (solo en development con errores reales)
  useEffect(() => {
    if (error && isMountedRef.current) {
      // Only log real errors, not development-related ones
      if (!(process.env.NODE_ENV === 'development' && 
            (error.includes('setRemoteDescription') || error.includes('m-lines')))) {
        Logger.error('Error actualizado', { error })
      }
    }
  }, [error])

  // Log cuando cambia el estado de stream
  useEffect(() => {
    if (isMountedRef.current) {
      Logger.debug('Estado del stream actualizado', { isStreamReady })
    }
  }, [isStreamReady])

  // Mantener el estado del micr√≥fono despu√©s de la navegaci√≥n
  useEffect(() => {
    if (isStreamReady && isMountedRef.current) {
      Logger.debug('Stream listo, verificando estado del micr√≥fono', { 
        isMicrophoneActive, 
        pathname 
      })
      
      // Recuperar estado del localStorage
      if (typeof window !== 'undefined') {
        const savedState = localStorage.getItem('microphoneActive') === 'true'
        
        Logger.debug('Estado del micr√≥fono en localStorage', { 
          savedState, 
          currentState: isMicrophoneActive 
        })
        
        // Si el estado guardado es diferente al actual, sincronizar
        if (savedState !== isMicrophoneActive) {
          Logger.info('Sincronizando estado del micr√≥fono con localStorage', { 
            savedState, 
            currentState: isMicrophoneActive 
          })
          setIsMicrophoneActive(savedState)
        }
        
        // Si el micr√≥fono deber√≠a estar activo pero no lo est√° en el RealtimeAgent
        if (savedState && streamingAgentRef.current) {
          const currentState = streamingAgentRef.current.isMicrophoneActive()
          Logger.debug('Verificando estado del micr√≥fono en RealtimeAgent', { 
            savedState, 
            currentState 
          })
          if (!currentState) {
            Logger.info('Reactivar micr√≥fono despu√©s de navegaci√≥n')
            streamingAgentRef.current.toggleMicrophone()
          }
        }
      }
    }
  }, [isStreamReady, pathname, isMicrophoneActive])

  return (
    <div className="h-full w-full flex items-end justify-center bg-white relative">
      {/* Show errors only in production or for real errors */}
      {error && process.env.NODE_ENV === 'production' && (
        <div className="absolute top-4 left-1/2 transform -translate-x-1/2 bg-red-500 text-white px-4 py-2 rounded-lg max-w-[80%] z-10">
          <p className="text-sm">{error}</p>
        </div>
      )}

      {/* Indicador de contexto */}
      <div className="absolute top-4 left-4 bg-blue-500 text-white px-3 py-1 rounded-full text-sm z-50">
        <div className="flex items-center gap-2">
          <span className="w-2 h-2 bg-white rounded-full"></span>
          <span>{currentContext.name}</span>
        </div>
      </div>

      {/* Indicador de navegaci√≥n */}
      {navigationDetected && (
        <div className="absolute top-4 right-4 bg-green-500 text-white px-4 py-2 rounded-lg text-sm z-50 animate-pulse">
          <div className="flex items-center gap-2">
            <svg className="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M13 7l5 5m0 0l-5 5m5-5H6" />
            </svg>
            <div>
              <div>Navegando a {navigationDetected.replace('/', '')}</div>
              <div className="text-xs opacity-90">Intenci√≥n detectada</div>
            </div>
          </div>
        </div>
      )}

      <RealtimeAgent
        ref={streamingAgentRef}
        contextId={currentContext.id}
        onStreamReady={handleStreamReady}
        onStreamError={handleStreamError}
      />

      {/* Bot√≥n de micr√≥fono */}
      {isStreamReady && (
        <div className="absolute bottom-6 right-6">
          <button
            onClick={handleMicrophoneToggle}
            className={`w-16 h-16 rounded-full flex items-center justify-center transition-all duration-200 ${
              isMicrophoneActive 
                ? 'bg-red-500 hover:bg-red-600 shadow-lg' 
                : 'bg-green-500 hover:bg-green-600 shadow-lg'
            }`}
            title={isMicrophoneActive ? 'Desactivar micr√≥fono' : 'Activar micr√≥fono'}
          >
            {isMicrophoneActive ? (
              <div className="flex flex-col items-center">
                <div className={`w-2 h-2 bg-white rounded-full mb-1 ${
                  isRecording ? 'animate-pulse' : ''
                }`}></div>
                <svg className="w-6 h-6 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                </svg>
              </div>
            ) : (
              <svg className="w-6 h-6 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
              </svg>
            )}
          </button>
          
          {/* Indicador de estado de grabaci√≥n */}
          {isRecording && (
            <div className="absolute -top-2 -right-2 bg-red-500 text-white text-xs px-2 py-1 rounded-full animate-pulse">
              Grabando
            </div>
          )}
        </div>
      )}

      {/* Grabador de audio para capturar voz del usuario */}
      <AudioRecorder
        onTranscription={handleUserTranscription}
        isActive={isMicrophoneActive}
        onRecordingStateChange={handleRecordingStateChange}
      />
    </div>
  )
}
